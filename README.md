[English](README_en.md) | **ä¸­æ–‡**

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)

# My LLM SDK

**ä¸€å¥—ä»£ç ï¼Œè°ƒç”¨å¤šå®¶æ¨¡å‹ã€‚**

> ç”¨åŒä¸€å¥— `client.generate()` / `stream()` è°ƒç”¨ Gemini / Qwen / OpenAI-compatibleã€‚  
> å†…ç½®é¢„ç®—æ§åˆ¶ã€429 è‡ªåŠ¨ç­‰å¾…é‡è¯•ã€Ledger è®°è´¦ä¸ç”¨é‡ç»Ÿè®¡ã€‚  
> é€‚åˆï¼šå›¢é˜Ÿå…±äº«æ¨¡å‹ç­–ç•¥ + ä¸ªäººæœ¬åœ° Key éš”ç¦» + éœ€è¦ç¨³å®šè·‘æ‰¹/é«˜å¹¶å‘/æˆæœ¬å¯è¿½è¸ªçš„åœºæ™¯ã€‚

---

## ğŸš€ å¿«é€Ÿä¸Šæ‰‹

### åœ¨ä½ çš„é¡¹ç›®ä¸­ä½¿ç”¨

```bash
# 1. å®‰è£…ï¼ˆä»æœ¬åœ°è·¯å¾„ï¼Œæœªæ¥æ”¯æŒ pip install my-llm-sdkï¼‰
pip install -e <SDKè·¯å¾„>/my-llm-sdk
# ä¾‹: pip install -e ~/projects/my-llm-sdk      (macOS/Linux)
#     pip install -e C:\Users\ä½ \my-llm-sdk     (Windows)

# 2. åœ¨ä½ çš„é¡¹ç›®ç›®å½•ä¸‹åˆå§‹åŒ–é…ç½®
python -m my_llm_sdk.cli init

# 3. ç¼–è¾‘ config.yamlï¼Œå¡«å…¥ API Key

# 4. è°ƒç”¨
python -m my_llm_sdk.cli generate --model gemini-2.5-flash --prompt "ä½ å¥½"
```

### å‚ä¸ SDK å¼€å‘

```bash
git clone https://github.com/NoneSeniorEngineer/my-llm-sdk.git
cd my-llm-sdk
pip install -e .
python -m my_llm_sdk.cli doctor
```

---

## ğŸ’¡ ä¸ºä»€ä¹ˆç”¨å®ƒ

| éœ€æ±‚ | My LLM SDK çš„è§£å†³æ–¹æ¡ˆ |
| :--- | :--- |
| **ä¸€æ¬¡æ¥å…¥ï¼Œå¤šå®¶åˆ‡æ¢** | ä¸æ”¹ä»£ç ï¼Œåªæ¢ `model_alias` |
| **æ€•è´¦å•å¤±æ§** | è¯·æ±‚å‰é¢„ç®—æ£€æŸ¥ + ç»Ÿä¸€ Ledger è®°è´¦ |
| **æ€• 429 / è¶…æ—¶** | è‡ªåŠ¨é€€é¿é‡è¯•ï¼Œå¯é…ç½®æœ€å¤§ç­‰å¾… |
| **å›¢é˜Ÿåä½œ** | `llm.project.yaml` (Git) + `config.yaml` (æœ¬åœ°) å½»åº•åˆ†ç¦» |
| **è·‘æ‰¹ / å¹¶å‘** | Async + Streaming + ç»“æ„åŒ–è¿”å›ï¼ˆcost/token ç»Ÿä¸€ï¼‰ |

---

## ğŸ§ª å…¸å‹ç”¨æ³•

### 1. è·‘æ‰¹ï¼šé¢„ç®—å°é¡¶ + è‡ªåŠ¨é‡è¯•
é€‚åˆ nightly job / æ•°æ®æ ‡æ³¨ / è¯„æµ‹è„šæœ¬ï¼šè¶…é¢„ç®—è‡ªåŠ¨æ‹’ç»ï¼Œ429 è‡ªåŠ¨ç­‰å¾…é‡è¯•ã€‚

### 2. åœ¨çº¿æœåŠ¡ï¼šStreaming + ç»Ÿä¸€ç”¨é‡ç»Ÿè®¡
`stream=True` æµå¼è¿”å›ï¼ŒåŒæ—¶ç²¾ç¡®è®°å½• token/cost åˆ° Ledgerã€‚

### 3. å›¢é˜Ÿåä½œï¼šç­–ç•¥å…±äº«ï¼ŒKey æ°¸ä¸å…¥åº“
`llm.project.yaml` æäº¤åˆ° Gitï¼›`config.yaml` åªåœ¨æœ¬åœ°ï¼ˆæ”¯æŒ personal overridesï¼‰ã€‚

---

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½ | è¯´æ˜ |
| :--- | :--- |
| **ç»Ÿä¸€æ¥å£** | ä¸€å¥— `client.generate()` è°ƒç”¨æ‰€æœ‰å‚å•† |
| **å¤šæ¨¡å‹æ”¯æŒ** | Gemini 2.5/3.0, Qwen Max/Plus/Flash, OpenAI Compatible |
| **Async + Streaming** | `generate_async` / `stream_async` æ”¯æŒé«˜å¹¶å‘ |
| **ç»“æ„åŒ–è¿”å›** | `full_response=True` è·å– usage/cost/token |
| **é¢„ç®—æ§åˆ¶** | æ¯æ¬¡è¯·æ±‚å‰æ£€æŸ¥æ¶ˆè´¹ï¼Œè¶…é¢è‡ªåŠ¨æ‹’ç» |
| **è‡ªåŠ¨é‡è¯•** | 429/è¶…æ—¶é€€é¿é‡è¯•ï¼Œå¯é…ç½® `max_retries` / `max_delay_s` |
| **åŒå±‚é…ç½®** | é¡¹ç›®è§„åˆ™ vs API Key åˆ†ç¦»ï¼Œé˜²æ­¢è¯¯æäº¤ |

---

## âœ… å¯é æ€§

- **è‡ªåŠ¨é‡è¯•**ï¼š429/è¶…æ—¶é€€é¿ï¼ˆå¯é…ç½®æœ€å¤§æ¬¡æ•°ä¸æœ€å¤§ç­‰å¾…æ—¶é—´ï¼‰
- **Ledger è®°è´¦**ï¼šæ¯æ¬¡è¯·æ±‚è®°å½• cost / token / provider / model / latency
- **ç»“æ„åŒ–è¿”å›**ï¼š`full_response=True` ç»Ÿä¸€æ‹¿åˆ° usage/cost
- **æµ‹è¯•è¦†ç›–**ï¼š`pytest` å•å…ƒæµ‹è¯• + ç«¯åˆ°ç«¯éªŒè¯è„šæœ¬

---

## ğŸ“¦ Python API

```python
from my_llm_sdk.client import LLMClient

client = LLMClient()

# åŸºç¡€è°ƒç”¨
response = client.generate("ä½ å¥½", model_alias="gemini-2.5-flash")
print(response)

# ç»“æ„åŒ–å¯¹è±¡ï¼ˆå« cost/tokenï¼‰
res = client.generate("ä½ å¥½", full_response=True)
print(f"Cost: ${res.cost}, Tokens: {res.usage.total_tokens}")

# æµå¼è¾“å‡º
for event in client.stream("æ•°åˆ°5", model_alias="qwen-max"):
    print(event.delta, end="", flush=True)
```

---

## ğŸ”§ é…ç½®å‚è€ƒ

### config.yamlï¼ˆæœ¬åœ°ï¼Œå‹¿æäº¤ Gitï¼‰
```yaml
api_keys:
  google: "AIzaSy..."
  dashscope: "sk-..."
daily_spend_limit: 5.0
```

### llm.project.yamlï¼ˆå¯æäº¤ Gitï¼‰
```yaml
model_registry:
  gemini-2.5-flash:
    provider: google
    model_id: gemini-2.5-flash
    rpm: 1000
```

### é‡è¯•é…ç½®
```yaml
resilience:
  max_retries: 3
  wait_on_rate_limit: true
  max_delay_s: 60
```

### æœ¬åœ°æ¨¡å‹è¦†ç›–ï¼ˆå¦‚ Ollamaï¼‰
```yaml
personal_model_overrides:
  llama-3-local:
    provider: openai
    model_id: llama3
    api_base: "http://localhost:11434/v1"
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡† (2025-12)

| æ¨¡å‹ | ç®€å•ä»»åŠ¡ | å¤æ‚ä»»åŠ¡ | å›ç­”é•¿åº¦ | ç‰¹ç‚¹ |
| :--- | :--- | :--- | :--- | :--- |
| qwen-flash | **3.70s** | 48.53s | 11414c | å“åº”æœ€å¿« |
| gemini-3.0-flash | 4.49s | **14.85s** | 5403c | å¤æ‚ä»»åŠ¡æœ€å¿« |
| qwen-plus | 3.95s | 33.15s | 7968c | ç®€å•ä»»åŠ¡æå¿« |
| gemini-2.5-pro | 16.47s | 53.80s | 9988c | æ·±åº¦æ€è€ƒ |
| qwen-max | 9.75s | 31.36s | 3822c | å›ç­”ç²¾ç‚¼ |

> **å¤ç°**ï¼š`python benchmark.py` (å¼€å‘æ¨¡å¼ä¸‹è¿è¡Œ)  
> **ç¯å¢ƒ**ï¼šmacOS + å®¶ç”¨ç½‘ç»œï¼Œä¸åŒåœ°åŒº/ç½‘ç»œå·®å¼‚å¤§  
> **ä»»åŠ¡å®šä¹‰**ï¼šSimple = å¸¸è¯†é—®ç­”ï¼›Complex = å¤šçº¿ç¨‹çˆ¬è™«ä»£ç ç”Ÿæˆ

---

## ğŸ—ºï¸ Roadmap

- [ ] å‘å¸ƒåˆ° PyPI (`pip install my-llm-sdk`)
- [ ] å¢åŠ  OpenTelemetry tracing
- [ ] æ›´å¤š OpenAI-compatible provider æ”¯æŒ
- [ ] å¤šæ¨¡æ€æ”¯æŒ (Vision / Audio)
- [ ] æ›´ç»†ç²’åº¦çš„æŒ‰ provider é”™è¯¯ç é‡è¯•ç­–ç•¥

---

## ğŸ¤ è´¡çŒ®

1. Fork æœ¬ä»“åº“
2. åœ¨ `src/my_llm_sdk/providers/` æ·»åŠ æ–° Providerï¼ˆç»§æ‰¿ `BaseProvider`ï¼‰
3. åœ¨ `src/my_llm_sdk/client.py` çš„ `self.providers` ä¸­æ³¨å†Œ
4. æäº¤ PR

---

## ğŸ“„ License

[Apache 2.0](LICENSE)
